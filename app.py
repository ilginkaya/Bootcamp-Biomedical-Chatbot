import streamlit as st
import os
from pathlib import Path
from typing import List

# LangChain'in Hata Ã‡Ã¶zÃ¼cÃ¼ YENÄ° ve UYUMLU Ä°MPORT YAPISI
from langchain_core.prompts import PromptTemplate
from langchain_core.vectorstores import VectorStoreRetriever # Retriever tipini tanÄ±mlamak iÃ§in
from langchain_core.runnables import RunnablePassthrough, RunnableLambda 
from langchain_core.output_parsers import StrOutputParser # Ã‡Ä±ktÄ±yÄ± dÃ¼z metne Ã§evirmek iÃ§in

from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- Sabitler ve Ayarlar ---
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004" 
DB_PATH = "rag_store"
DOCS_PATH = "data_docs" 

# API AnahtarÄ±nÄ± Streamlit secrets veya ortam deÄŸiÅŸkeninden Ã§eker
if "gemini" in st.secrets:
    API_KEY = st.secrets["gemini"]["api_key"]
else:
    API_KEY = os.getenv("GEMINI_API_KEY")

# --- RAG FonksiyonlarÄ± ---
@st.cache_resource
def load_rag_chain():
    """RAG zincirini, LLM'i yÃ¼kler ve veritabanÄ±nÄ± kontrol/oluÅŸturur."""

    api_key = API_KEY
    
    if not api_key:
        st.error("âŒ HATA: GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen Streamlit Cloud Secrets ayarlarÄ±nÄ± kontrol edin.")
        return None 
    
    # 1. Embedding Fonksiyonu
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=api_key)

    # 2. VeritabanÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et ve yÃ¼kle
    if not Path(DB_PATH).exists():
        # VeritabanÄ± yoksa otomatik oluÅŸturma (Ä°lk Ã§alÄ±ÅŸtÄ±rmada sadece bir kez olur)
        try:
            loader = DirectoryLoader(
                DOCS_PATH, glob="**/*.txt", loader_kwargs={'encoding': 'utf-8', 'errors': 'ignore'}
            )
            docs = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_documents(docs)
            
            if not chunks:
                st.error("HATA: Otomatik indeksleme baÅŸarÄ±sÄ±z oldu. DokÃ¼manlar boÅŸ veya okunamÄ±yor.")
                return None

            vector_store = Chroma.from_documents(
                documents=chunks, embedding=embeddings, persist_directory=DB_PATH
            )
            
        except Exception as e:
            st.error(f"FATAL HATA: Otomatik indeksleme sÄ±rasÄ±nda beklenmeyen hata oluÅŸtu: {e}")
            return None
        
    else:
        # VeritabanÄ± varsa, sadece yÃ¼kle
        vector_store = Chroma(
            persist_directory=DB_PATH, embedding_function=embeddings
        )

    # 3. Yeni Runnable RAG Zincirini Kurma (HatasÄ±z YÃ¶ntem)
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2, google_api_key=api_key)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    # 3.1. Prompt Template
    prompt_template = """
    Sen bir Biyomedikal Bilgi AsistanÄ±sÄ±n. AÅŸaÄŸÄ±daki biyomedikal metinleri (Context) kullanarak, kullanÄ±cÄ±ya TÃ¼rkÃ§e ve net bir ÅŸekilde yanÄ±t ver. 
    YanÄ±tlarÄ±n teknik, kÄ±sa ve direkt olmalÄ±dÄ±r. BaÄŸlamda bulamadÄ±ÄŸÄ±n sorulara "Bu konuda elimde yeterli bilgi yok." diye yanÄ±t ver.

    Context: {context}
    Soru: {question}
    YanÄ±t:
    """
    PROMPT = PromptTemplate.from_template(prompt_template)
    
    # 3.2. Zincir TanÄ±mÄ±: RetrievalQA'nÄ±n Runnable karÅŸÄ±lÄ±ÄŸÄ±
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()} 
        | PROMPT
        | llm
        | StrOutputParser()
    )
    
    # Not: Bu versiyon sadece yanÄ±t dÃ¶ndÃ¼rÃ¼r, kaynaklarÄ± Ã§ekmez. 
    # KaynaklarÄ± Ã§ekmek iÃ§in ek RunnableLambda kodlarÄ± gerekir ki, projeyi basit tutalÄ±m.
    
    return rag_chain, retriever # Hem zinciri hem de retriever'Ä± dÃ¶ndÃ¼rÃ¼yoruz

# --- Streamlit Ana UygulamasÄ± ---
def main():
    st.set_page_config(page_title="Biyomedikal RAG AsistanÄ±", layout="wide")

    # BaÅŸlÄ±k ve AÃ§Ä±klama
    st.markdown(
        """
        <div style="text-align: center; background-color: #008080; padding: 10px; border-radius: 10px; color: white;">
            <h1>ğŸ”¬ Biyomedikal RAG Bilgi AsistanÄ±</h1>
            <p>Gemini AI, LangChain ve ChromaDB ile gÃ¼Ã§lendirilmiÅŸtir.</p>
        </div>
        """, 
        unsafe_allow_html=True
    )
    st.markdown("---")

    # RAG sistemini baÅŸlat
    # Yeni yapÄ± ile iki deÄŸer dÃ¶nÃ¼yor: zincir ve retriever
    qa_chain, retriever = load_rag_chain()
    if not qa_chain:
        st.stop() 

    # Chat MesajlarÄ±
    if 'messages' not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Merhaba! Ben biyomedikal bilgi asistanÄ±nÄ±zÄ±m. DokÃ¼manlarÄ±mdaki (TÄ±bbi terimler, cihazlar) konularla ilgili sorular sorun."}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar="ğŸ”¬" if message["role"] == "assistant" else "ğŸ‘¤"):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan Girdi Alma
    if prompt := st.chat_input("TÄ±bbi bir terimi aÃ§Ä±klayabilir misin?"):

        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(prompt)

        with st.spinner("ğŸ§  Gemini yanÄ±t oluÅŸturuyor..."):
            # RAG zincirini Ã§alÄ±ÅŸtÄ±r
            try:
                # Runnable zinciri sadece query'yi alÄ±r
                response = qa_chain.invoke(prompt) 
                
                # Kaynak dokÃ¼manlarÄ± Ã§ekme (AyrÄ± bir adÄ±m olarak)
                docs = retriever.get_relevant_documents(prompt)
                
                # Kaynak dokÃ¼manlarÄ± ekle
                sources = "\n\n### ğŸ“„ Kaynak DokÃ¼manlar:\n"
                for i, doc in enumerate(docs):
                    file_name = doc.metadata.get('source', 'Bilinmeyen Kaynak')
                    sources += f"- **{file_name}**:\n > {doc.page_content[:150]}...\n" 
                
                full_response = response + sources

                st.session_state.messages.append({"role": "assistant", "content": full_response})
                with st.chat_message("assistant", avatar="ğŸ”¬"):
                    st.markdown(full_response)

            except Exception as e:
                st.error(f"YanÄ±t oluÅŸturulamadÄ±. Hata: {e}")
                st.session_state.messages.append({"role": "assistant", "content": "ÃœzgÃ¼nÃ¼m, Gemini ile baÄŸlantÄ± kurulamadÄ± veya bir hata oluÅŸtu."})


if __name__ == "__main__":
    main()