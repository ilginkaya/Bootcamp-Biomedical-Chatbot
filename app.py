import os
from pathlib import Path
import gradio as gr # Yeni Gradio kÃ¼tÃ¼phanesi
from typing import List

# LangChain ve Gemini KÃ¼tÃ¼phaneleri (TÃ¼m uyumlu importlar)
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- Sabitler ve Ayarlar ---
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004"
DB_PATH = "rag_store"
DOCS_PATH = "data_docs"

# API AnahtarÄ±nÄ± al (Lokalde ve Cloud'da Ã§alÄ±ÅŸÄ±r)
API_KEY = os.getenv("GEMINI_API_KEY")

# --- RAG Zinciri Kurulumu (load_rag_chain) ---
# Gradio'da Streamlit'in @st.cache_resource yerine normal bir fonskiyon kullanÄ±yoruz
def load_rag_chain():
    """RAG zincirini, LLM'i yÃ¼kler ve veritabanÄ±nÄ± kontrol/oluÅŸturur."""

    if not API_KEY:
        # Gradio'da hata mesajÄ± yerine None dÃ¶ndÃ¼rÃ¼yoruz
        print("HATA: GEMINI_API_KEY ortam deÄŸiÅŸkeni ayarlanmadÄ±.")
        return None, None
    
    # 1. Embedding Fonksiyonu
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=API_KEY)

    # 2. VeritabanÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et ve oluÅŸtur (YalnÄ±zca yoksa)
    if not Path(DB_PATH).exists():
        try:
            # DokÃ¼manlarÄ± yÃ¼kleme
            loader = DirectoryLoader(
                DOCS_PATH, glob="**/*.txt", loader_kwargs={'encoding': 'utf-8', 'errors': 'ignore'}
            )
            docs = loader.load()
            
            # ParÃ§alara ayÄ±rma (Chunking)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_documents(docs)
            
            if not chunks:
                print("HATA: Otomatik indeksleme baÅŸarÄ±sÄ±z oldu.")
                return None, None

            vector_store = Chroma.from_documents(
                documents=chunks, embedding=embeddings, persist_directory=DB_PATH
            )
            print("VeritabanÄ± baÅŸarÄ±yla oluÅŸturuldu.")
            
        except Exception as e:
            print(f"FATAL HATA: Otomatik indeksleme sÄ±rasÄ±nda beklenmeyen hata oluÅŸtu: {e}")
            return None, None
        
    else:
        # VeritabanÄ± varsa, sadece yÃ¼kle
        vector_store = Chroma(
            persist_directory=DB_PATH, embedding_function=embeddings
        )

    # 3. RAG Zincirini Kurma
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2, google_api_key=API_KEY)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    prompt_template = """
    Sen bir Biyomedikal Bilgi AsistanÄ±sÄ±n. AÅŸaÄŸÄ±daki biyomedikal metinleri (Context) kullanarak, kullanÄ±cÄ±ya TÃ¼rkÃ§e ve net bir ÅŸekilde yanÄ±t ver. 
    YanÄ±tlarÄ±n teknik, kÄ±sa ve direkt olmalÄ±dÄ±r. BaÄŸlamda bulamadÄ±ÄŸÄ±n sorulara 'Bu konuda elimde yeterli bilgi yok.' diye yanÄ±t ver.

    Context: {context}
    Soru: {question}
    YanÄ±t:
    """
    PROMPT = PromptTemplate.from_template(prompt_template)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain, retriever 

# RAG zincirini bir kez baÅŸlat
QA_CHAIN, RETRIEVER = load_rag_chain()

# --- Gradio ArayÃ¼z Fonksiyonu ---
def predict(query):
    if not QA_CHAIN:
        return "API AnahtarÄ± bulunamadÄ±ÄŸÄ± iÃ§in chatbot ÅŸu anda Ã§alÄ±ÅŸmÄ±yor."
    
    try:
        # RAG zincirini Ã§alÄ±ÅŸtÄ±rma
        result = QA_CHAIN({"query": query})
        response = result['result']
        docs = result['source_documents']

        # Kaynak dokÃ¼manlarÄ± formatlama
        sources = "\n\n### ğŸ“„ Kaynaklar:\n"
        for i, doc in enumerate(docs):
            file_name = doc.metadata.get('source', 'Bilinmeyen Kaynak')
            sources += f"- **{file_name}**:\n > {doc.page_content[:100]}...\n" 
        
        return response + sources

    except Exception as e:
        return f"YanÄ±t oluÅŸturulamadÄ±. LÃ¼tfen API anahtarÄ±nÄ±zÄ± kontrol edin. Hata: {e}"

# --- Gradio ArayÃ¼z TanÄ±mÄ± ---
if __name__ == "__main__":
    if QA_CHAIN:
        # Gradio arayÃ¼zÃ¼nÃ¼ tanÄ±mla
        iface = gr.Interface(
            fn=predict,
            inputs=gr.Textbox(lines=2, placeholder="Biyomedikal sorunuzu giriniz...", label="Sorunuzu Buraya YazÄ±nÄ±z"), 
            outputs="markdown",
            title="ğŸ”¬ Biyomedikal RAG Bilgi AsistanÄ±",
            description="Gemini, LangChain ve 14 biyomedikal bilgi dosyasÄ± ile gÃ¼Ã§lendirilmiÅŸtir."
        )
        iface.launch(share=True) # Public link oluÅŸturmak iÃ§in share=True
    else:
        print("\nChatbot baÅŸlatÄ±lamadÄ±. LÃ¼tfen API anahtarÄ± ayarÄ±nÄ±zÄ± kontrol edin.")