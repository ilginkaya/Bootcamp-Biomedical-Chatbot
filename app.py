import streamlit as st
import os
from pathlib import Path
from typing import List

# LangChain'in Hata Ã‡Ã¶zÃ¼cÃ¼ ve UYUMLU Ä°MPORT YAPISI
from langchain_core.prompts import PromptTemplate 
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA # LangChain'in son yapÄ±sÄ± burada kaldÄ±

# Ã‡ekirdek ve Topluluk ModÃ¼lleri
from langchain_core.output_parsers import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter # TextSplitter'Ä± LangChain'den Ã§ekiyoruz
from langchain_community.vectorstores import Chroma 
from langchain_community.document_loaders import DirectoryLoader
from langchain_core.runnables import RunnablePassthrough # Runnable yapÄ±sÄ± iÃ§in


# --- Sabitler ve Ayarlar ---
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004" 
DB_PATH = "rag_store"
DOCS_PATH = "data_docs" 

# API AnahtarÄ±nÄ± Streamlit secrets veya ortam deÄŸiÅŸkeninden Ã§eker
if "gemini" in st.secrets:
    API_KEY = st.secrets["gemini"]["api_key"]
else:
    API_KEY = os.getenv("GEMINI_API_KEY")

# --- RAG FonksiyonlarÄ± ---
@st.cache_resource
def load_rag_chain():
    """RAG zincirini, LLM'i yÃ¼kler ve veritabanÄ±nÄ± kontrol/oluÅŸturur."""

    api_key = API_KEY
    
    if not api_key:
        st.error("âŒ HATA: GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen Streamlit Cloud Secrets ayarlarÄ±nÄ± kontrol edin.")
        return None 
    
    # 1. Embedding Fonksiyonu
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=api_key)

    # 2. VeritabanÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et ve yÃ¼kle
    vector_store = None
    
    if not Path(DB_PATH).exists():
        # --- VERÄ°TABANI YOKSA OTOMATÄ°K OLUÅTURMA BAÅLANGICI ---
        try:
            # DokÃ¼manlarÄ± yÃ¼kleme
            loader = DirectoryLoader(
                DOCS_PATH, glob="**/*.txt", loader_kwargs={'encoding': 'utf-8', 'errors': 'ignore'}
            )
            docs = loader.load()
            
            # ParÃ§alara ayÄ±rma (Chunking)
            # TextSplitter'Ä± hatasÄ±z yerden Ã§ekiyoruz.
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_documents(docs)
            
            if not chunks:
                st.error("HATA: Otomatik indeksleme baÅŸarÄ±sÄ±z oldu. DokÃ¼manlar boÅŸ veya okunamÄ±yor.")
                return None

            vector_store = Chroma.from_documents(
                documents=chunks, embedding=embeddings, persist_directory=DB_PATH
            )
            
        except Exception as e:
            st.error(f"FATAL HATA: Otomatik indeksleme sÄ±rasÄ±nda beklenmeyen hata oluÅŸtu: {e}")
            return None
        
    else:
        # VeritabanÄ± varsa, sadece yÃ¼kle
        vector_store = Chroma(
            persist_directory=DB_PATH, embedding_function=embeddings
        )

    # 3. RAG Zincirini Kurma
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2, google_api_key=api_key)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    prompt_template = """
    Sen bir Biyomedikal Bilgi AsistanÄ±sÄ±n. AÅŸaÄŸÄ±daki biyomedikal metinleri (Context) kullanarak, kullanÄ±cÄ±ya TÃ¼rkÃ§e ve net bir ÅŸekilde yanÄ±t ver. 
    YanÄ±tlarÄ±n teknik, kÄ±sa ve direkt olmalÄ±dÄ±r. BaÄŸlamda bulamadÄ±ÄŸÄ±n sorulara "Bu konuda elimde yeterli bilgi yok." diye yanÄ±t ver.

    Context: {context}
    Soru: {question}
    YanÄ±t:
    """
    PROMPT = PromptTemplate.from_template(prompt_template)

    # RetrievalQA, LangChain'in kendi iÃ§indeki en stabil ve basit zincir yapÄ±sÄ±dÄ±r.
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain, retriever 

# --- Streamlit Ana UygulamasÄ± ---
def main():
    st.set_page_config(page_title="Biyomedikal RAG AsistanÄ±", layout="wide")

    # BaÅŸlÄ±k ve AÃ§Ä±klama
    st.markdown(
        """
        <div style="text-align: center; background-color: #008080; padding: 10px; border-radius: 10px; color: white;">
            <h1>ğŸ”¬ Biyomedikal RAG Bilgi AsistanÄ±</h1>
            <p>Gemini AI, LangChain ve ChromaDB ile gÃ¼Ã§lendirilmiÅŸtir.</p>
        </div>
        """, 
        unsafe_allow_html=True
    )
    st.markdown("---")

    # RAG sistemini baÅŸlat
    qa_chain, retriever = load_rag_chain()
    if not qa_chain:
        st.stop() 

    # Chat MesajlarÄ±
    if 'messages' not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Merhaba! Ben biyomedikal bilgi asistanÄ±nÄ±zÄ±m. DokÃ¼manlarÄ±mdaki (TÄ±bbi terimler, cihazlar) konularla ilgili sorular sorun."}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar="ğŸ”¬" if message["role"] == "assistant" else "ğŸ‘¤"):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan Girdi Alma
    if prompt := st.chat_input("TÄ±bbi bir terimi aÃ§Ä±klayabilir misin?"):

        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(prompt)

        with st.spinner("ğŸ§  Gemini yanÄ±t oluÅŸturuyor..."):
            # RAG zincirini Ã§alÄ±ÅŸtÄ±r
            try:
                # RetrievalQA'yÄ± kullanÄ±yoruz
                result = qa_chain({"query": prompt})
                response = result['result']
                
                # Kaynak dokÃ¼manlarÄ± doÄŸrudan result'tan Ã§ekiyoruz (return_source_documents=True ayarlandÄ±ÄŸÄ± iÃ§in)
                docs = result['source_documents']

                # Kaynak dokÃ¼manlarÄ± ekle
                sources = "\n\n### ğŸ“„ Kaynak DokÃ¼manlar:\n"
                for i, doc in enumerate(docs):
                    file_name = doc.metadata.get('source', 'Bilinmeyen Kaynak')
                    sources += f"- **{file_name}**:\n > {doc.page_content[:150]}...\n" 
                
                full_response = response + sources

                st.session_state.messages.append({"role": "assistant", "content": full_response})
                with st.chat_message("assistant", avatar="ğŸ”¬"):
                    st.markdown(full_response)

            except Exception as e:
                st.error(f"YanÄ±t oluÅŸturulamadÄ±. Hata: {e}")
                st.session_state.messages.append({"role": "assistant", "content": "ÃœzgÃ¼nÃ¼m, Gemini ile baÄŸlantÄ± kurulamadÄ± veya bir hata oluÅŸtu."})


if __name__ == "__main__":
    main()